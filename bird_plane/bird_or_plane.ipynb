{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 3072) (4800, 3072)\n"
     ]
    }
   ],
   "source": [
    "train_x = pd.read_csv('./bora/train_x.csv', index_col=0, header=None)\n",
    "train_y = pd.read_csv('./bora/train_y.csv', index_col=0)\n",
    "test_x = pd.read_csv('./bora/test_x.csv', index_col=0, header=None)\n",
    "print(train_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappping_type = {'Bird': 0, 'Airplane': 1}\n",
    "train_y = train_y.replace({\"target\": mappping_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_x = train_x.values.reshape(7200,32,32,3)\n",
    "_test_x = test_x.values.reshape(4800,32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = \\\n",
    "        train_test_split(_train_x, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5760, 32, 32, 3) (5760, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3634\n",
       "0    3566\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Flatten, Activation, Conv2D, MaxPooling2D, Dropout \n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    shear_range=0.1\n",
    "#     brightness_range\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5760, 32, 32, 3) (5760, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_train = np.array(np_utils.to_categorical(y_train.target.values, num_classes=2))\n",
    "y_cat_val = np.array(np_utils.to_categorical(y_val.target.values, num_classes=2))\n",
    "y_cat_all = np.array(np_utils.to_categorical(train_y.target.values, num_classes=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(32, (5, 5), input_shape=(32, 32, 3), padding='same', activation='relu', \n",
    "                  kernel_constraint=maxnorm(3)) )\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(32, (5, 5), padding='same', activation='relu', kernel_constraint=maxnorm(3)) )\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv2D(64, (5, 5), padding='same', activation='relu', kernel_constraint=maxnorm(3)) )\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(64, (5, 5), padding='same', activation='relu', kernel_constraint=maxnorm(3)) )\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv2D(64, (5, 5), padding='same', activation='relu', kernel_constraint=maxnorm(3)) )\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv2D(64, (5, 5), padding='same', activation='relu', kernel_constraint=maxnorm(3)) )\n",
    "cnn.add(Conv2D(64, (5, 5), padding='valid', activation='relu', kernel_constraint=maxnorm(3)) )\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)) )\n",
    "cnn.add(Conv2D(32, (3, 3), padding='valid', activation='relu', kernel_constraint=maxnorm(3)) )\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(256, activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dense(64, activation='relu'))\n",
    "cnn.add(Dense(32, activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dense(2, activation='softmax'))\n",
    "\n",
    "adam = Adam()\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "# print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.2959 - acc: 0.8743\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.3076 - acc: 0.8669\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.3146 - acc: 0.8645\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2948 - acc: 0.8780\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.2993 - acc: 0.8761\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2871 - acc: 0.8781\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2815 - acc: 0.8822\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2992 - acc: 0.8755\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.3011 - acc: 0.8733\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.2799 - acc: 0.8849\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.2815 - acc: 0.8819\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2834 - acc: 0.8806\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2775 - acc: 0.8893\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2696 - acc: 0.8895\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2719 - acc: 0.8874\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.2509 - acc: 0.8945\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.2480 - acc: 0.8993\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2574 - acc: 0.8958\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2798 - acc: 0.8821\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.2565 - acc: 0.8920\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.2700 - acc: 0.8855\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2640 - acc: 0.8899\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2470 - acc: 0.9008\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2415 - acc: 0.8991\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2546 - acc: 0.8951\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2465 - acc: 0.8978\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.2524 - acc: 0.8953\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.2327 - acc: 0.9044\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2525 - acc: 0.8986\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2282 - acc: 0.9058\n"
     ]
    }
   ],
   "source": [
    "history = cnn.fit_generator(datagen.flow(x_train, y_cat_train, batch_size=250), \n",
    "                              steps_per_epoch=100, epochs=30, \n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8979166666666667"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(cnn.predict(x_val),axis=1), np.argmax(y_cat_val, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "import keras\n",
    "\n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(32, 32, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "90/90 [==============================] - 5s 59ms/step - loss: 0.8049 - acc: 0.7439\n",
      "Epoch 2/250\n",
      "90/90 [==============================] - 2s 28ms/step - loss: 0.7003 - acc: 0.7790\n",
      "Epoch 3/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.6511 - acc: 0.8036\n",
      "Epoch 4/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.6131 - acc: 0.8033\n",
      "Epoch 5/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.5819 - acc: 0.8174\n",
      "Epoch 6/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.5639 - acc: 0.8137\n",
      "Epoch 7/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.5270 - acc: 0.8269\n",
      "Epoch 8/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.5271 - acc: 0.8248\n",
      "Epoch 9/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.4876 - acc: 0.8330\n",
      "Epoch 10/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.4707 - acc: 0.8465\n",
      "Epoch 11/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.4671 - acc: 0.8429\n",
      "Epoch 12/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.4604 - acc: 0.8420\n",
      "Epoch 13/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.4449 - acc: 0.8512\n",
      "Epoch 14/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.4362 - acc: 0.8439\n",
      "Epoch 15/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.4211 - acc: 0.8517\n",
      "Epoch 16/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.4273 - acc: 0.8490\n",
      "Epoch 17/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.4208 - acc: 0.8526\n",
      "Epoch 18/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.4047 - acc: 0.8556: 1\n",
      "Epoch 19/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3963 - acc: 0.8618\n",
      "Epoch 20/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3947 - acc: 0.8568\n",
      "Epoch 21/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3915 - acc: 0.8583\n",
      "Epoch 22/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3844 - acc: 0.8627\n",
      "Epoch 23/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3818 - acc: 0.8670\n",
      "Epoch 24/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3837 - acc: 0.8578\n",
      "Epoch 25/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3791 - acc: 0.8615\n",
      "Epoch 26/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3668 - acc: 0.8663\n",
      "Epoch 27/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3777 - acc: 0.8656\n",
      "Epoch 28/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3798 - acc: 0.8667\n",
      "Epoch 29/250\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.3675 - acc: 0.869 - 2s 27ms/step - loss: 0.3659 - acc: 0.8708\n",
      "Epoch 30/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3485 - acc: 0.8788\n",
      "Epoch 31/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3628 - acc: 0.8686\n",
      "Epoch 32/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3518 - acc: 0.8740: 0s - loss: 0.3579 - acc: \n",
      "Epoch 33/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3581 - acc: 0.8731\n",
      "Epoch 34/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3497 - acc: 0.8743: 1s - lo\n",
      "Epoch 35/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3504 - acc: 0.8830\n",
      "Epoch 36/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3507 - acc: 0.8764\n",
      "Epoch 37/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3383 - acc: 0.8844\n",
      "Epoch 38/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3520 - acc: 0.8778\n",
      "Epoch 39/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3530 - acc: 0.8773\n",
      "Epoch 40/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3375 - acc: 0.8854\n",
      "Epoch 41/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3377 - acc: 0.8807\n",
      "Epoch 42/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3528 - acc: 0.8766\n",
      "Epoch 43/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3436 - acc: 0.8774\n",
      "Epoch 44/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3323 - acc: 0.8898\n",
      "Epoch 45/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3361 - acc: 0.8868\n",
      "Epoch 46/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3347 - acc: 0.8905\n",
      "Epoch 47/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3367 - acc: 0.8851\n",
      "Epoch 48/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3383 - acc: 0.8832\n",
      "Epoch 49/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3331 - acc: 0.8851\n",
      "Epoch 50/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3219 - acc: 0.8957\n",
      "Epoch 51/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3385 - acc: 0.8840\n",
      "Epoch 52/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3475 - acc: 0.8783\n",
      "Epoch 53/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3155 - acc: 0.8967: 1s - loss: 0.3077 - acc: - ETA: 1s -\n",
      "Epoch 54/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3275 - acc: 0.8944\n",
      "Epoch 55/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.3280 - acc: 0.8901\n",
      "Epoch 56/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3200 - acc: 0.8955\n",
      "Epoch 57/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3263 - acc: 0.8899\n",
      "Epoch 58/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.3213 - acc: 0.8998\n",
      "Epoch 59/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3230 - acc: 0.8939\n",
      "Epoch 60/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3101 - acc: 0.9000: 0s - loss: 0.319\n",
      "Epoch 61/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3140 - acc: 0.8988: 0s - loss: 0.3271 - \n",
      "Epoch 62/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3216 - acc: 0.8953\n",
      "Epoch 63/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3274 - acc: 0.8918\n",
      "Epoch 64/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3245 - acc: 0.8898\n",
      "Epoch 65/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3231 - acc: 0.8931\n",
      "Epoch 66/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.3104 - acc: 0.8971\n",
      "Epoch 67/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3252 - acc: 0.8920: 1\n",
      "Epoch 68/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.3172 - acc: 0.8981\n",
      "Epoch 69/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3228 - acc: 0.8953\n",
      "Epoch 70/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3222 - acc: 0.8974\n",
      "Epoch 71/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3140 - acc: 0.8991\n",
      "Epoch 72/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3207 - acc: 0.8955\n",
      "Epoch 73/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3115 - acc: 0.9003\n",
      "Epoch 74/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3049 - acc: 0.9054\n",
      "Epoch 75/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.3142 - acc: 0.8991\n",
      "Epoch 76/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.3130 - acc: 0.8981\n",
      "Epoch 77/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.2909 - acc: 0.9050\n",
      "Epoch 78/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2854 - acc: 0.9085\n",
      "Epoch 79/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2865 - acc: 0.9059\n",
      "Epoch 80/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2768 - acc: 0.9094\n",
      "Epoch 81/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2797 - acc: 0.9111\n",
      "Epoch 82/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2754 - acc: 0.9141\n",
      "Epoch 83/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2762 - acc: 0.9128\n",
      "Epoch 84/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2795 - acc: 0.9118\n",
      "Epoch 85/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2696 - acc: 0.9146\n",
      "Epoch 86/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2609 - acc: 0.9212\n",
      "Epoch 87/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2855 - acc: 0.9116\n",
      "Epoch 88/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2645 - acc: 0.9174\n",
      "Epoch 89/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2737 - acc: 0.9116: 1s - loss: 0.2843 - acc: 0.90 - ETA: 1s - loss: 0.2862 - acc: 0. - ETA: 0s - loss: 0.28\n",
      "Epoch 90/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2518 - acc: 0.9233\n",
      "Epoch 91/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2684 - acc: 0.9135\n",
      "Epoch 92/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2772 - acc: 0.9120\n",
      "Epoch 93/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2652 - acc: 0.9179\n",
      "Epoch 94/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2663 - acc: 0.9181\n",
      "Epoch 95/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2616 - acc: 0.9186\n",
      "Epoch 96/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2631 - acc: 0.9214\n",
      "Epoch 97/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2603 - acc: 0.9212\n",
      "Epoch 98/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2623 - acc: 0.9151\n",
      "Epoch 99/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2610 - acc: 0.9212\n",
      "Epoch 100/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2600 - acc: 0.9215\n",
      "Epoch 101/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2639 - acc: 0.9194\n",
      "Epoch 102/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2449 - acc: 0.9238\n",
      "Epoch 103/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2478 - acc: 0.9247\n",
      "Epoch 104/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2459 - acc: 0.9267\n",
      "Epoch 105/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2421 - acc: 0.9243\n",
      "Epoch 106/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2472 - acc: 0.9200: 1s - l\n",
      "Epoch 107/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2270 - acc: 0.9318\n",
      "Epoch 108/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2451 - acc: 0.9215\n",
      "Epoch 109/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2465 - acc: 0.9219\n",
      "Epoch 110/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2326 - acc: 0.9281\n",
      "Epoch 111/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2470 - acc: 0.9226\n",
      "Epoch 112/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2350 - acc: 0.9286\n",
      "Epoch 113/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2366 - acc: 0.9241\n",
      "Epoch 114/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2342 - acc: 0.9273\n",
      "Epoch 115/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2290 - acc: 0.9325: 0s - loss: 0.2\n",
      "Epoch 116/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2303 - acc: 0.9314\n",
      "Epoch 117/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2311 - acc: 0.9302\n",
      "Epoch 118/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.2399 - acc: 0.9255: 0s - loss: 0.2380 - acc:\n",
      "Epoch 119/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2338 - acc: 0.9276\n",
      "Epoch 120/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2369 - acc: 0.9285\n",
      "Epoch 121/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2196 - acc: 0.9323\n",
      "Epoch 122/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2291 - acc: 0.9319\n",
      "Epoch 123/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2322 - acc: 0.9318\n",
      "Epoch 124/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2235 - acc: 0.9323\n",
      "Epoch 125/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2343 - acc: 0.9264\n",
      "Epoch 126/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2238 - acc: 0.9345\n",
      "Epoch 127/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.2310 - acc: 0.9273\n",
      "Epoch 128/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2275 - acc: 0.9266\n",
      "Epoch 129/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2258 - acc: 0.9330: 0s - loss: 0.2242 - acc: 0.93\n",
      "Epoch 130/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2225 - acc: 0.9323\n",
      "Epoch 131/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2167 - acc: 0.9342\n",
      "Epoch 132/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2267 - acc: 0.9335\n",
      "Epoch 133/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2208 - acc: 0.9316\n",
      "Epoch 134/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.2142 - acc: 0.9396\n",
      "Epoch 135/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2128 - acc: 0.9380\n",
      "Epoch 136/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2322 - acc: 0.9323\n",
      "Epoch 137/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2254 - acc: 0.9321\n",
      "Epoch 138/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.2138 - acc: 0.9378\n",
      "Epoch 139/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2202 - acc: 0.9361\n",
      "Epoch 140/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2188 - acc: 0.9328\n",
      "Epoch 141/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2226 - acc: 0.9347: 1s - loss: \n",
      "Epoch 142/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2184 - acc: 0.9306\n",
      "Epoch 143/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.2214 - acc: 0.9332\n",
      "Epoch 144/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2157 - acc: 0.9351\n",
      "Epoch 145/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2149 - acc: 0.9332\n",
      "Epoch 146/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2203 - acc: 0.9299\n",
      "Epoch 147/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2219 - acc: 0.9347\n",
      "Epoch 148/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2107 - acc: 0.9396\n",
      "Epoch 149/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2079 - acc: 0.9351\n",
      "Epoch 150/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2214 - acc: 0.9335\n",
      "Epoch 151/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2139 - acc: 0.9352\n",
      "Epoch 152/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2227 - acc: 0.9313\n",
      "Epoch 153/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2140 - acc: 0.9328\n",
      "Epoch 154/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2141 - acc: 0.9394\n",
      "Epoch 155/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2161 - acc: 0.9345\n",
      "Epoch 156/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2142 - acc: 0.9372\n",
      "Epoch 157/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2103 - acc: 0.9372\n",
      "Epoch 158/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2017 - acc: 0.9403\n",
      "Epoch 159/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2086 - acc: 0.9380\n",
      "Epoch 160/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2188 - acc: 0.9342\n",
      "Epoch 161/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2145 - acc: 0.9378\n",
      "Epoch 162/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2107 - acc: 0.9361\n",
      "Epoch 163/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2135 - acc: 0.9359\n",
      "Epoch 164/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2051 - acc: 0.9403\n",
      "Epoch 165/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1956 - acc: 0.9420\n",
      "Epoch 166/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2094 - acc: 0.9365\n",
      "Epoch 167/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2151 - acc: 0.9344: 1s - loss: \n",
      "Epoch 168/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2115 - acc: 0.9405\n",
      "Epoch 169/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2094 - acc: 0.9370\n",
      "Epoch 170/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2078 - acc: 0.9365\n",
      "Epoch 171/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2139 - acc: 0.9363\n",
      "Epoch 172/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2056 - acc: 0.9378\n",
      "Epoch 173/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2169 - acc: 0.9347\n",
      "Epoch 174/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1974 - acc: 0.9425\n",
      "Epoch 175/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2202 - acc: 0.9323\n",
      "Epoch 176/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2135 - acc: 0.9352\n",
      "Epoch 177/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2104 - acc: 0.9385\n",
      "Epoch 178/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2052 - acc: 0.9399\n",
      "Epoch 179/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2037 - acc: 0.9389\n",
      "Epoch 180/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.1995 - acc: 0.9436\n",
      "Epoch 181/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2097 - acc: 0.9358\n",
      "Epoch 182/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2072 - acc: 0.9392\n",
      "Epoch 183/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2014 - acc: 0.9431\n",
      "Epoch 184/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2026 - acc: 0.9413\n",
      "Epoch 185/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2146 - acc: 0.9335\n",
      "Epoch 186/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2131 - acc: 0.9372\n",
      "Epoch 187/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.1975 - acc: 0.9415\n",
      "Epoch 188/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2057 - acc: 0.9385\n",
      "Epoch 189/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1999 - acc: 0.9392\n",
      "Epoch 190/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2136 - acc: 0.9373\n",
      "Epoch 191/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2111 - acc: 0.9344\n",
      "Epoch 192/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2075 - acc: 0.9391\n",
      "Epoch 193/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2011 - acc: 0.9434\n",
      "Epoch 194/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2029 - acc: 0.9399\n",
      "Epoch 195/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2011 - acc: 0.9417\n",
      "Epoch 196/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2072 - acc: 0.9387\n",
      "Epoch 197/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2080 - acc: 0.9370\n",
      "Epoch 198/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2089 - acc: 0.9339\n",
      "Epoch 199/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2015 - acc: 0.9403\n",
      "Epoch 200/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1973 - acc: 0.9420\n",
      "Epoch 201/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2007 - acc: 0.9427\n",
      "Epoch 202/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.1964 - acc: 0.9457\n",
      "Epoch 203/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1975 - acc: 0.9424\n",
      "Epoch 204/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1875 - acc: 0.9476\n",
      "Epoch 205/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.2057 - acc: 0.9413\n",
      "Epoch 206/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1967 - acc: 0.9399\n",
      "Epoch 207/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.2013 - acc: 0.9403\n",
      "Epoch 208/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.1893 - acc: 0.9450: \n",
      "Epoch 209/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1995 - acc: 0.9443\n",
      "Epoch 210/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1965 - acc: 0.9422\n",
      "Epoch 211/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1982 - acc: 0.9431\n",
      "Epoch 212/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2019 - acc: 0.9420\n",
      "Epoch 213/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1979 - acc: 0.9429\n",
      "Epoch 214/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1921 - acc: 0.9441\n",
      "Epoch 215/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1985 - acc: 0.9417\n",
      "Epoch 216/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1940 - acc: 0.9441\n",
      "Epoch 217/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1931 - acc: 0.9429\n",
      "Epoch 218/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2041 - acc: 0.9398\n",
      "Epoch 219/250\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 0.1949 - acc: 0.9424\n",
      "Epoch 220/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1862 - acc: 0.9477\n",
      "Epoch 221/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2003 - acc: 0.9451\n",
      "Epoch 222/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1884 - acc: 0.9470\n",
      "Epoch 223/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1931 - acc: 0.9424\n",
      "Epoch 224/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1930 - acc: 0.9420\n",
      "Epoch 225/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2063 - acc: 0.9411\n",
      "Epoch 226/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1940 - acc: 0.9434\n",
      "Epoch 227/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1974 - acc: 0.9436\n",
      "Epoch 228/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1954 - acc: 0.9434\n",
      "Epoch 229/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1987 - acc: 0.9404: 0s - loss: 0.1935 - acc\n",
      "Epoch 230/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1883 - acc: 0.9453\n",
      "Epoch 231/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1887 - acc: 0.9474\n",
      "Epoch 232/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1903 - acc: 0.9502\n",
      "Epoch 233/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1949 - acc: 0.9420\n",
      "Epoch 234/250\n",
      "90/90 [==============================] - 2s 25ms/step - loss: 0.1829 - acc: 0.9498\n",
      "Epoch 235/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1862 - acc: 0.9457\n",
      "Epoch 236/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2048 - acc: 0.9391\n",
      "Epoch 237/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1893 - acc: 0.9464\n",
      "Epoch 238/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1895 - acc: 0.9470\n",
      "Epoch 239/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1902 - acc: 0.9498\n",
      "Epoch 240/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1986 - acc: 0.9415\n",
      "Epoch 241/250\n",
      "90/90 [==============================] - 3s 29ms/step - loss: 0.1947 - acc: 0.9453\n",
      "Epoch 242/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1851 - acc: 0.9472\n",
      "Epoch 243/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.2021 - acc: 0.9403\n",
      "Epoch 244/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1914 - acc: 0.9476\n",
      "Epoch 245/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1860 - acc: 0.9484\n",
      "Epoch 246/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1927 - acc: 0.9472\n",
      "Epoch 247/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1918 - acc: 0.9453\n",
      "Epoch 248/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1974 - acc: 0.9422\n",
      "Epoch 249/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1831 - acc: 0.9503\n",
      "Epoch 250/250\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 0.1913 - acc: 0.9464\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "history = model.fit_generator(datagen.flow(_train_x, y_cat_all, batch_size=batch_size), \n",
    "                              steps_per_epoch=x_train.shape[0] // batch_size, epochs=150, \n",
    "                              shuffle=True, callbacks=[LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(model.predict(x_val),axis=1), np.argmax(y_cat_val, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model.h5') \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4575619e-03 9.9754250e-01]\n",
      " [6.0797322e-01 3.9202675e-01]\n",
      " [9.9868304e-01 1.3169284e-03]\n",
      " [1.8196836e-01 8.1803161e-01]\n",
      " [3.5764121e-02 9.6423584e-01]\n",
      " [3.6147574e-03 9.9638522e-01]\n",
      " [1.2516640e-03 9.9874830e-01]\n",
      " [9.0967995e-01 9.0320021e-02]\n",
      " [1.7343678e-04 9.9982661e-01]\n",
      " [6.9514210e-03 9.9304855e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(np.array([[i, x.argmax()] for i, x in enumerate(prediction)]), columns=['id', 'target'])\n",
    "\n",
    "mappping_type_inv = {0: 'Bird', 1: 'Airplane'}\n",
    "sample = sample.replace({'target': mappping_type_inv})\n",
    "sample.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GrayScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_img(df, i):\n",
    "    plt.imshow(df.values.reshape(df.shape[0], 32, 32, 3)[i])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def viz_img_gray(df, i):\n",
    "    img = rgb2gray(df)[i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "def rgb2gray(rgb):\n",
    "    rgb = rgb.values.reshape(rgb.shape[0], 32, 32, 3)\n",
    "    return np.matmul(rgb, [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz_img_gray(train_x, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_in_gray = rgb2gray(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 3072) (7200, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_in_gray.shape)\n",
    "train_in_gray = train_in_gray.reshape(7200, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_matrix, test_feature_matrix, train_labels, test_labels = \\\n",
    "        train_test_split(train_in_gray, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_matrix, test_feature_matrix = train_feature_matrix/255, test_feature_matrix/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "clf.fit(train_feature_matrix, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6097222222222223"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(clf.predict(test_feature_matrix), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(loss='log', penalty='l2')\n",
    "param_grid={\n",
    "    'max_iter':np.linspace(100, 10000, 10),\n",
    "    'tol': np.linspace(0.0001, 0.01, 10)\n",
    "    }\n",
    "gs = GridSearchCV(clf, param_grid=param_grid, cv=3,\n",
    "                n_jobs=-1, scoring='accuracy') \n",
    "gs.fit(train_feature_matrix, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=8900.0,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=0.0089,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss='log', penalty='l2')\n",
    "clf.fit(train_feature_matrix, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_clf = LogisticRegression(solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid={\n",
    "    'C': np.linspace(0.01, 1, 4), \n",
    "    'penalty': ['l1', 'l2']\n",
    "    }\n",
    "gs = GridSearchCV(lr_clf, param_grid=param_grid, cv=3,\n",
    "                n_jobs=-1, scoring='accuracy') \n",
    "gs.fit(train_feature_matrix, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7680555555555556"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr_clf = LogisticRegression(penalty='l2', C=0.01, solver='saga')\n",
    "\n",
    "best_lr_clf.fit(train_feature_matrix, train_labels)\n",
    "accuracy_score(best_lr_clf.predict(test_feature_matrix), test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr_clf = LogisticRegression(penalty='l2', C=0.01, solver='saga')\n",
    "\n",
    "best_lr_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y_lr = best_lr_clf.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(np.array([[i, x.argmax()] for i, x in enumerate(predict_y)]), columns=['id', 'target'])\n",
    "\n",
    "mappping_type_inv = {0: 'Bird', 1: 'Airplane'}\n",
    "sample = sample.replace({'target': mappping_type_inv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Airplane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    target\n",
       "0   0  Airplane\n",
       "1   1  Airplane\n",
       "2   2  Airplane\n",
       "3   3  Airplane\n",
       "4   4  Airplane"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n"
     ]
    }
   ],
   "source": [
    "ans = pd.read_csv('submit.csv', index_col=0)\n",
    "print(len(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "k = list(range(1, 3, 3))\n",
    "param_grid={\n",
    "    'n_neighbors': k,\n",
    "    'weights': ['distance', 'uniform']\n",
    "    }\n",
    "gs = GridSearchCV(knn, param_grid=param_grid, cv=3,\n",
    "                n_jobs=-1, scoring='accuracy')\n",
    "gs.fit(train_feature_matrix, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_knn = KNeighborsClassifier(n_neighbors=1, weights='distance')\n",
    "best_knn.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631944444444444"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(best_knn.predict(test_feature_matrix), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y_knn = best_knn.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.special import binom\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits as load\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_matrix, test_feature_matrix, train_labels, test_labels = \\\n",
    "        train_test_split(train_x, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree: 0.7107648321554599\n"
     ]
    }
   ],
   "source": [
    "d3 = DecisionTreeClassifier() #   \n",
    "\n",
    "print(\"Decision tree:\", cross_val_score(d3,train_feature_matrix, train_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:621: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:621: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:621: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging: 0.7824634300509491\n"
     ]
    }
   ],
   "source": [
    "print(\"Bagging:\", cross_val_score(BaggingClassifier(d3), train_feature_matrix, train_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:621: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:621: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:621: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Bagging: 0.7895821200237986\n"
     ]
    }
   ],
   "source": [
    "f = train_feature_matrix.shape[1]\n",
    "rnd_d3 = DecisionTreeClassifier(max_features=int(f ** 0.5)) #      \n",
    "\n",
    "print(\"Randomized Bagging:\", cross_val_score(BaggingClassifier(rnd_d3), train_feature_matrix, train_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3.fit(train_x, train_y)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d3 = DecisionTreeClassifier()\n",
    "param_grid={\n",
    "    'criterion':  ['gini', 'entropy'],\n",
    "    'max_depth':['2', '5','10', 'None']\n",
    "    }\n",
    "accuracy = []\n",
    "# for k, criterion in enumerate(['gini', 'entropy']):\n",
    "for i, max_depth in enumerate([10, 15, 20, None]):\n",
    "    for j, min_samples_leaf in enumerate([15, 5, 1]):\n",
    "        \n",
    "        clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, criterion=\"gini\")\n",
    "        clf.fit(train_feature_matrix, train_labels)\n",
    "        accuracy.append(accuracy_score(clf.predict(test_feature_matrix), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d3 = DecisionTreeClassifier(criterion=\"gini\")\n",
    "#bag_d3 = BaggingClassifier(ds)\n",
    "accuracy = []\n",
    "for i, n_estimators in enumerate([ 1000]):\n",
    "    for j, max_samples in enumerate([1]):\n",
    "        bag_d3 = BaggingClassifier(d3, n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples)\n",
    "        bag_d3.fit(train_feature_matrix, train_labels.values.ravel())\n",
    "        accuracy.append((i,j,accuracy_score(bag_d3.predict(test_feature_matrix), test_labels.values.ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0.5027777777777778), 0.7458333333333333]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "for i, max_depth in enumerate([2, 5, 10, None]):\n",
    "    for j, min_samples_leaf in enumerate([15, 5, 1]):\n",
    "        \n",
    "        clf = DecisionTreeClassifier(max_depth=10, min_samples_leaf=5, criterion=\"entropy\")\n",
    "        clf.fit(train_feature_matrix, train_labels)\n",
    "        accuracy.append(accuracy_score(clf.predict(test_feature_matrix), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10 ,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=25, n_estimators=1000, n_jobs=-1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3 = DecisionTreeClassifier(criterion=\"gini\")\n",
    "d3 = BaggingClassifier(d3, n_jobs=-1, n_estimators=1000, max_samples=25)\n",
    "d3.fit(train_x, train_y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = DecisionTreeClassifier(criterion=\"gini\")\n",
    "d3 = BaggingClassifier(d3, n_jobs=-1, n_estimators=1000, max_samples=25)\n",
    "d3.fit(test_feature_matrix, test_labels.values.ravel())\n",
    "accuracy.append(accuracy_score(d3.predict(test_feature_matrix), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y_d3 = d3.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.377 0.623]\n"
     ]
    }
   ],
   "source": [
    "print(predict_y_d3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = (predict_y_d3 * 0.7458 + predict_y_knn * 0.7632 + predict_y_lr * 0.768) / (0.7458 + 0.7632 + 0.768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15372921 0.84627079]\n"
     ]
    }
   ],
   "source": [
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_y = best_lr_clf.predict_proba(test_x)\n",
    "\n",
    "sample = pd.DataFrame(np.array([[i, x.argmax()] for i, x in enumerate(prediction)]), columns=['id', 'target'])\n",
    "\n",
    "mappping_type_inv = {0: 'Bird', 1: 'Airplane'}\n",
    "sample = sample.replace({'target': mappping_type_inv})\n",
    "sample.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
